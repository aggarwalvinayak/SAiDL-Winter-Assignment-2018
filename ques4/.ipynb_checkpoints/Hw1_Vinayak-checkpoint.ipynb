{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinayak/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import mujoco_py\n",
    "import gym\n",
    "import load_policy\n",
    "import pickle\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent(torch.nn.Module):\n",
    "    def __init__(self,dim,learning_rate=0.01):\n",
    "        super(Agent,self).__init__()\n",
    "        self.model=torch.nn.Sequential(\n",
    "        #Write MODEL\n",
    "            torch.nn.Linear(dim[0],150),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(150,50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(50,dim[1])\n",
    "        \n",
    "        )  ##one forward step of the neural net\n",
    "        self.loss_fn=torch.nn.MSELoss()#check what is use of size average\n",
    "        self.history=[]\n",
    "        self.rewards=[]\n",
    "        self.optimizer= torch.optim.Adam(self.model.parameters(),lr=learning_rate)\n",
    "        \n",
    "        \n",
    "    def train(self,training_data,batch_size,epochs):\n",
    "        m=training_data[\"observations\"].shape[0] #no of training examples\n",
    "        loss=[]\n",
    "        X=torch.from_numpy(training_data[\"observations\"]).float()\n",
    "        Y=torch.from_numpy(training_data[\"actions\"]).float()\n",
    "        for iter_no in range(epochs):\n",
    "            for j in range(0,m,batch_size):\n",
    "                loss.append(self.train_step(Variable(X[j:j+batch_size]),Variable(Y[j:j+batch_size])))    \n",
    "            self.history.append(np.mean(loss))\n",
    "    \n",
    "    def train_step(self,X,Y):\n",
    "        y_pred=self.model(X)\n",
    "        loss=self.loss_fn(y_pred,Y)  \n",
    "        self.optimizer.zero_grad()  \n",
    "        loss.backward()  #calculates gradient\n",
    "        self.optimizer.step()  #changes weight according to gradients\n",
    "        return loss.item()\n",
    "    \n",
    "    def act(self,obs):\n",
    "        obs=torch.from_numpy(np.expand_dims(obs,0)).float()\n",
    "        return self.model(obs)\n",
    "        \n",
    "    def record_reward(self,reward):\n",
    "        self.rewards.append(np.mean(reward))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Expert:\n",
    "    def __init__(self,expert_policy_file):\n",
    "        self.policy_fn=load_policy.load_policy(expert_policy_file)\n",
    "    def act(self,obs):\n",
    "        return self.policy_fn(obs[None,:]) #policy_fn returns the action... None adds another blank axis \n",
    "    def record_reward(self, reward):\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Simulator():  #to generaate training data by running expert\n",
    "    def __init__(self,envname):\n",
    "        self.initialize_env(envname)\n",
    "        self.envname=envname\n",
    "    \n",
    "    def initialize_env(self,envname):\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        self.env=gym.make(envname)\n",
    "        \n",
    "    def simulate(self,agent,max_steps,num_rollouts,render):\n",
    "        with self.session.as_default():\n",
    "            returns=[]\n",
    "            observations=[]\n",
    "            rewards=[]\n",
    "            actions=[]\n",
    "            for i in range(num_rollouts):\n",
    "                obs=self.env.reset()\n",
    "                done=False\n",
    "                total_reward=0\n",
    "                steps=0\n",
    "                while not done:\n",
    "                    action=agent.act(obs)\n",
    "                    observations.append(obs)\n",
    "                    actions.append(action)\n",
    "                    obs,reward,done,info=self.env.step(action)\n",
    "                    total_reward+=reward\n",
    "                    steps+=1\n",
    "                    if render:\n",
    "                        self.env.render()\n",
    "                    if steps>=max_steps:\n",
    "                        break\n",
    "                returns.append(total_reward)\n",
    "            print('Return summary: mean=',np.mean(returns),\"  std=\",np.std(returns))\n",
    "            agent.record_reward(returns)\n",
    "            return (observations, actions)\n",
    "     \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arguments():\n",
    "    parser=argparse.ArgumentParser()\n",
    "    parser.add_argument(\"expert_policy_file\", type=str)\n",
    "    parser.add_argument(\"envname\", type=str)\n",
    "    parser.add_argument(\"--render\", action='store_true')\n",
    "    parser.add_argument(\"--max_timesteps\", type=int, default=500)\n",
    "    parser.add_argument(\"--num_rollouts\",type=int,default=1) #no of expert rollouts\n",
    "    \n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #args=arguments()\n",
    "# # class DotDict(dict):\n",
    "# #     def __getattr__(self, name):\n",
    "# #         return self[name]\n",
    "args ={\n",
    "    'batch_size': 128,  # Number of training data for each epoch\n",
    "    'epochs': 50, # Number of epoch in training the model\n",
    "    'envname':'Humanoid-v2',  # Environment to stimulate the expert\n",
    "    'expert_policy_file' : './experts/Humanoid-v2.pkl',  # Read expert from file\n",
    "    'num_rollouts' : 5,  # Number of rollouts to play for each iter of training\n",
    "    'render' : False,  # Whether to render the final training result in animation\n",
    "    'max_timesteps' : 100000,  # Stop the env after this number of steps being taken\n",
    "    }\n",
    "\n",
    "# expert= Expert(args['expert_policy_file'])\n",
    "\n",
    "# simulator = Simulator(args['envname'])\n",
    "# training_data= simulator.simulate(expert,max_steps=args[\"max_timesteps\"],num_rollouts=args[\"num_rollouts\"],render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/Humanoid-v2.pkl', 'rb') as f:\n",
    "    training_data = pickle.load(f)\n",
    "#print(training_data.keys())\n",
    "#print(training_data['observations'].shape[-1],training_data['actions'].shape[-1])\n",
    "#training_data=Variable(training_data)\n",
    "#training_data=torch.from_numpy(np.array(training_data))\n",
    "dim=(training_data['observations'].shape[-1],training_data['actions'].shape[-1])\n",
    "\n",
    "#print(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-59ae40b8027f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "source": [
    "agent = Agent(dim)\n",
    "agent.train(training_data,batch_size=args['batch_size'],epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(agent.history)\n",
    "\n",
    "def gen_new_plot(title, ylabel, xlabel):\n",
    "    fig, ax = plt.subplots( nrows=1, ncols=1 )\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    return ax\n",
    "\n",
    "\n",
    "ax1 = gen_new_plot('Training Loss', 'Train Loss', 'Epoch')\n",
    "ax1.plot(agent.history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.state_dict(), \"./data/trained_data3.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(dim)\n",
    "agent.load_state_dict(torch.load(\"./data/trained_data3.pkl\"))\n",
    "print(trained_agent.env.spec.timestep_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    trained_agent=Simulator(args['envname'])\n",
    "    data = trained_agent.simulate(agent,max_steps=10000000,num_rollouts=10,render=True)\n",
    "    #trained_agent.glfw.destroy_window()\n",
    "    print(trained_agent.env.spec.timestep_limit)\n",
    "    trained_agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
